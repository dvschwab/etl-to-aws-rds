# An Amazon RDS MySQL Instance: Design and Implementation

Amazon's Relational Database Service (RDS) is a straightforward, yet powerful way to transition database projects to the cloud. Establish workflows are largely preserved, while Amazon handles database administration: maintaining the file system, provisioning backups, and applying software updates and security fixes. They offer different sizes from the tiny 1 core, 1 GiB RAM db.t2.micro on the Free Tier to the more realistic db.m5.4xlarge (16 cores, 64GiB RAM). Auto-scaling is also available for high-availibity databases with variable useage patterns.

Of course, none of this is free: the moderately powerful db.m5.4xlarge costs $1.37 per instance hour, or $986 for 24/7 access each month. Ensuring reliability requires hosting the database in multiple Availability Zones, doubling (or tripling) the cost. Automated backups, snapshots, and logs are other expenses. Without a dedicated commitment to cost management, the savings advantage of using cloud services is easily lost to these extraneous expenses. 

Nevertheless, the cloud is here to stay; now is the time to make nice, and configuring a SQL database using Amazon's RDS service is an excellent starter project.

## Repository Contents

This repository contains code to provision, configure, and load data into a MySQL instance hosted by Amazon's Relational Database Service (RDS). To control costs, the tiny instance db.t2.micro was used. The data itself is also a small, somewhat silly list of Pokemon attributes. Nevertheless, this project simulates actual business workflows, addressing data preparation, schema normalization, views, and stored procedures. The project also uses Amazon's Virtual Private Cloud (VPC) and security groups to control network access and their Identity and Access Management (IAM) service for role-based authentication.

The database itself is a protoype data warehouse with fact and dimension table arranged in a standard star schema. The schema includes both one-to-many and many-to-many relations, and is generated by a series of SQL scripts. The data itself is a CSV file containing descriptions and statistics of 802 Pokemon (an amusing departure from the seriousness of business data). For the ETL workflow, Python scripts are used for initial data processing. The data is loaded into the staging table with an additional script, and a stored procedure loads the data from the staging table into the normalized production tables.

## AWS Considerations

While hosting a database through Amazon's RDS service is not difficult, there are several differences from traditional hosting to be aware of. In addition, proper network and security configuration is handled entirely in the cloud with Amazon services. There are also differences in database operation: the "root" account is in fact a pseudo-root without CREATE TABLESPACE and SUPER permissions and the database contains additional service tables used by AWS for database management. While it makes sense for Amazon to control the aspects, these differences may require modification of established business practices.

### Networking and Security

It takes several steps to establish a secure connection to any RDS instance.

1. Create a VPC (Virtual Private Cloud) to house the database
2. Create an Internet Gateway within the VPC and assign an Elastic IP Address
3. Configure a security group associated with this VPC
4. Create a database access role with appropriate privileges

Once steps 1 and 2 are done, users may connect to the database at the IP address assigned to the Internet Gateway using their database credentials (i.e. the MySQL-based login and password). After step 3, the user must be allowed by the security group to connect; typically, this is done by allowing a certain IP address range. Step 4 requires the user be authorized for the IAM credentials governing the RDS instance. This may seem overkill, but is really no different from the way network and role access is allocated outside the cloud.provided the security group allows inbound connections from their IP address range. 

Once connected, the user will have access to whatever resources and permissions their MySQL role allows: in other words, security of the database instance is done through the Amazon-provided VPC and security group, while security of database tables and assigned privileges are done through the MySQL client.

## Data Preparation

The data used was a small CSV file containing statistics for 802 Pokemon. These include its abilities, attack strength, hit points, and speed. Notwithstanding the small number of records, this data set posed several challenges to load into a normalized SQL database. These are:

* Each record contained several kanji characters (i.e. Japanese characters) that could not be processed by MySQL's LOAD command. 
* Abilities for each record were recorded as a variable length array delimited by brackets (i.e. '[' and ']'). 
* The relationship between Pokemon and their abilities was many-to-many: each Pokemon could posess one or more abilities, and each ability could be used by several different Pokemon.

The first and second challenges were address with brief Python scripts. The last challenge required a crosswalk table to mediate the relationship between the Pokemon themselves and the table of abilities. The relavant logic from the script that removed characters is shown below:

```python
def remove_kanji(input):
    
	input_filtered = []
    for item in input:

        # Split on end bracket to separate abilities from the rest
        item_row = item.split(b']"')

        # Split the remaining stats on comma, then delete kanji field

        abilities = item_row[0].decode() + ']"'
        stats = item_row[1].decode().split(',')
        del stats[KANJI_FIELD]

        # Join everything into a single list

        input_filtered.append(abilities + '|'.join(stats))

    return input_filtered
```

## Database Definition

The database is named pokemon and generated via SQL scripts. There are 5 tables; one of these is the staging table, which loads the data unaltered, while the remaining tables are the fact table (statistics), two dimension tables (ability and species), and the crosswalk table that joins the statistics to the ability table.

![](Images/pokemon_EER_model_image.png")

### Stored Procedures

To transfer the staging data to the normalized tables, the stored procedure load_data_from_stage was written. This procedure used a cursor to insert each row of data from the staging table into the statistics table. The procedure then retrieved the last inserted ID and inserted it, along with the ability IDs of the abilities the Pokemon could use, into the crosswalk table. Because the abilities in the staging table were stored in a pseudo-array, the procedure used the INSTR() function on the abilities field to determine the appropriate abilities for each row, and then selected the ability_ID from the abilities table corresponding to the selected values. This generated each set of entries into the crosswalk table without needing to split the abilities field in the staging table into individual elements. The code that performed this operation is shown below.

```SQL read_loop: loop
		fetch stage_cursor into pokemon_name, pokemon_abilities, pokemon_type;
		if done then 
			leave read_loop;
		end if;
		
		-- Get the species_id from the species table corresponding to the Pokemon's type
		-- so we can load that instead of the type
		
		select species_id from species where species_type = pokemon_type into new_species_id;
		
		-- Insert next pokemon into statistics table
		
		insert into statistics (english_name, classification, ... )
		select english_name, classification, ... from stage where english_name = pokemon_name;
	
		-- Insert the pokemon ID and ability_ids into the xwalk.
	
		set new_pokemon_id = last_insert_id();
	
		insert into statistics_to_abilities_xwalk (pokemon_id_FK, ability_id_FK)
		select new_pokemon_id, ability_id from abilities where instr(pokemon_abilities, ability);

		end loop;
```
Since the species field was normalized as a separate table, the cursor also inserted the appropriate species_id into the statistics table by selecting the species_ID from the species table corresponding to the Pokemon's species in the staging table (where the field is named type1). The relevant code for this operation is shown below.

## Summary

This project created a normalized database from a small data set describing Pokemon abilities and statistics. The normalized schema contains four tables: the primary data table statistics, which functions as the fact table; the dimension table species; the dimension table abilities; and the crosswalk table linking the statistics data to the abilities table. The fact table contains 802 rows, each representing a distinct Pokemon. The normalized schema allows a wide range of queries to be performed, including queries not easily performed against the raw data set, such as retrieving the Pokemon able to use each ability. Useful queries may easily be stored as views, and the statistics table may be further normalized by creating new relationship tables for the remaining feature dimensions.

The database itself is provided as in instance of Amazon's Relational Database Service. Securing access required both networking and policy services (i.e. the Virtual Private Cloud and Identity and Access Management services) as well as database-specific roles and permissions. Security groups were also established, and keypairs generated for programmatic access via the RDS SDK and Amazon CLI.

While the database itself is relatively small, the steps outlined here may easily be used to manage access to much larger data sets suitable for a SQL environment. For such data sets, indices and partitioning may easily be performed to minimize query time. Hopefully, the brief discussion presented here will encourage people to try Amazon's RDS service for similar projects.
